2025-11-06 01:05:44,352: Requested GPU 1 not available. Using GPU 0 instead.
2025-11-06 01:05:44,537: Using device: cuda:0
2025-11-06 01:30:13,909: Requested GPU 1 not available. Using GPU 0 instead.
2025-11-06 01:30:14,163: Using device: cuda:0
2025-11-06 01:30:15,391: Using torch.compile
2025-11-06 01:30:19,997: Initialized RNN decoding model
2025-11-06 01:30:19,997: OptimizedModule(
  (_orig_mod): GRUDecoder(
    (day_layer_activation): Softsign()
    (day_weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 512x512]
        (1): Parameter containing: [torch.float32 of size 512x512]
        (2): Parameter containing: [torch.float32 of size 512x512]
        (3): Parameter containing: [torch.float32 of size 512x512]
        (4): Parameter containing: [torch.float32 of size 512x512]
        (5): Parameter containing: [torch.float32 of size 512x512]
        (6): Parameter containing: [torch.float32 of size 512x512]
        (7): Parameter containing: [torch.float32 of size 512x512]
        (8): Parameter containing: [torch.float32 of size 512x512]
        (9): Parameter containing: [torch.float32 of size 512x512]
        (10): Parameter containing: [torch.float32 of size 512x512]
        (11): Parameter containing: [torch.float32 of size 512x512]
        (12): Parameter containing: [torch.float32 of size 512x512]
        (13): Parameter containing: [torch.float32 of size 512x512]
        (14): Parameter containing: [torch.float32 of size 512x512]
        (15): Parameter containing: [torch.float32 of size 512x512]
        (16): Parameter containing: [torch.float32 of size 512x512]
        (17): Parameter containing: [torch.float32 of size 512x512]
        (18): Parameter containing: [torch.float32 of size 512x512]
        (19): Parameter containing: [torch.float32 of size 512x512]
        (20): Parameter containing: [torch.float32 of size 512x512]
        (21): Parameter containing: [torch.float32 of size 512x512]
        (22): Parameter containing: [torch.float32 of size 512x512]
        (23): Parameter containing: [torch.float32 of size 512x512]
        (24): Parameter containing: [torch.float32 of size 512x512]
        (25): Parameter containing: [torch.float32 of size 512x512]
        (26): Parameter containing: [torch.float32 of size 512x512]
        (27): Parameter containing: [torch.float32 of size 512x512]
        (28): Parameter containing: [torch.float32 of size 512x512]
        (29): Parameter containing: [torch.float32 of size 512x512]
        (30): Parameter containing: [torch.float32 of size 512x512]
        (31): Parameter containing: [torch.float32 of size 512x512]
        (32): Parameter containing: [torch.float32 of size 512x512]
        (33): Parameter containing: [torch.float32 of size 512x512]
        (34): Parameter containing: [torch.float32 of size 512x512]
        (35): Parameter containing: [torch.float32 of size 512x512]
        (36): Parameter containing: [torch.float32 of size 512x512]
        (37): Parameter containing: [torch.float32 of size 512x512]
        (38): Parameter containing: [torch.float32 of size 512x512]
        (39): Parameter containing: [torch.float32 of size 512x512]
        (40): Parameter containing: [torch.float32 of size 512x512]
        (41): Parameter containing: [torch.float32 of size 512x512]
        (42): Parameter containing: [torch.float32 of size 512x512]
        (43): Parameter containing: [torch.float32 of size 512x512]
        (44): Parameter containing: [torch.float32 of size 512x512]
    )
    (day_biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1x512]
        (1): Parameter containing: [torch.float32 of size 1x512]
        (2): Parameter containing: [torch.float32 of size 1x512]
        (3): Parameter containing: [torch.float32 of size 1x512]
        (4): Parameter containing: [torch.float32 of size 1x512]
        (5): Parameter containing: [torch.float32 of size 1x512]
        (6): Parameter containing: [torch.float32 of size 1x512]
        (7): Parameter containing: [torch.float32 of size 1x512]
        (8): Parameter containing: [torch.float32 of size 1x512]
        (9): Parameter containing: [torch.float32 of size 1x512]
        (10): Parameter containing: [torch.float32 of size 1x512]
        (11): Parameter containing: [torch.float32 of size 1x512]
        (12): Parameter containing: [torch.float32 of size 1x512]
        (13): Parameter containing: [torch.float32 of size 1x512]
        (14): Parameter containing: [torch.float32 of size 1x512]
        (15): Parameter containing: [torch.float32 of size 1x512]
        (16): Parameter containing: [torch.float32 of size 1x512]
        (17): Parameter containing: [torch.float32 of size 1x512]
        (18): Parameter containing: [torch.float32 of size 1x512]
        (19): Parameter containing: [torch.float32 of size 1x512]
        (20): Parameter containing: [torch.float32 of size 1x512]
        (21): Parameter containing: [torch.float32 of size 1x512]
        (22): Parameter containing: [torch.float32 of size 1x512]
        (23): Parameter containing: [torch.float32 of size 1x512]
        (24): Parameter containing: [torch.float32 of size 1x512]
        (25): Parameter containing: [torch.float32 of size 1x512]
        (26): Parameter containing: [torch.float32 of size 1x512]
        (27): Parameter containing: [torch.float32 of size 1x512]
        (28): Parameter containing: [torch.float32 of size 1x512]
        (29): Parameter containing: [torch.float32 of size 1x512]
        (30): Parameter containing: [torch.float32 of size 1x512]
        (31): Parameter containing: [torch.float32 of size 1x512]
        (32): Parameter containing: [torch.float32 of size 1x512]
        (33): Parameter containing: [torch.float32 of size 1x512]
        (34): Parameter containing: [torch.float32 of size 1x512]
        (35): Parameter containing: [torch.float32 of size 1x512]
        (36): Parameter containing: [torch.float32 of size 1x512]
        (37): Parameter containing: [torch.float32 of size 1x512]
        (38): Parameter containing: [torch.float32 of size 1x512]
        (39): Parameter containing: [torch.float32 of size 1x512]
        (40): Parameter containing: [torch.float32 of size 1x512]
        (41): Parameter containing: [torch.float32 of size 1x512]
        (42): Parameter containing: [torch.float32 of size 1x512]
        (43): Parameter containing: [torch.float32 of size 1x512]
        (44): Parameter containing: [torch.float32 of size 1x512]
    )
    (day_layer_dropout): Dropout(p=0.2, inplace=False)
    (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.4)
    (out): Linear(in_features=768, out_features=41, bias=True)
  )
)
2025-11-06 01:30:19,999: Model has 44,315,177 parameters
2025-11-06 01:30:20,000: Model has 11,819,520 day-specific parameters | 26.67% of total parameters
2025-11-06 01:30:44,173: Successfully initialized datasets
2025-11-06 01:30:54,080: Train batch 0: loss: 753.88 grad norm: 307.47 time: 3.648
2025-11-06 01:30:54,080: Running test after training batch: 0
2025-11-06 01:31:17,421: Val batch 0: PER (avg): 1.2153 CTC Loss (avg): 714.9868 time: 23.341
2025-11-06 01:31:17,423: t15.2023.08.13 val PER: 1.1268
2025-11-06 01:31:17,424: t15.2023.08.18 val PER: 1.1467
2025-11-06 01:31:17,424: t15.2023.08.20 val PER: 1.1517
2025-11-06 01:31:17,424: t15.2023.08.25 val PER: 1.1642
2025-11-06 01:31:17,424: t15.2023.08.27 val PER: 1.0675
2025-11-06 01:31:17,425: t15.2023.09.01 val PER: 1.2232
2025-11-06 01:31:17,425: t15.2023.09.03 val PER: 1.1152
2025-11-06 01:31:17,425: t15.2023.09.24 val PER: 1.3313
2025-11-06 01:31:17,426: t15.2023.09.29 val PER: 1.2412
2025-11-06 01:31:17,426: t15.2023.10.01 val PER: 1.0555
2025-11-06 01:31:17,426: t15.2023.10.06 val PER: 1.2433
2025-11-06 01:31:17,426: t15.2023.10.08 val PER: 1.0433
2025-11-06 01:31:17,426: t15.2023.10.13 val PER: 1.1606
2025-11-06 01:31:17,426: t15.2023.10.15 val PER: 1.2030
2025-11-06 01:31:17,427: t15.2023.10.20 val PER: 1.3557
2025-11-06 01:31:17,427: t15.2023.10.22 val PER: 1.2973
2025-11-06 01:31:17,427: t15.2023.11.03 val PER: 1.2985
2025-11-06 01:31:17,428: t15.2023.11.04 val PER: 1.3857
2025-11-06 01:31:17,428: t15.2023.11.17 val PER: 1.6470
2025-11-06 01:31:17,429: t15.2023.11.19 val PER: 1.4311
2025-11-06 01:31:17,429: t15.2023.11.26 val PER: 1.2565
2025-11-06 01:31:17,429: t15.2023.12.03 val PER: 1.1838
2025-11-06 01:31:17,429: t15.2023.12.08 val PER: 1.2543
2025-11-06 01:31:17,429: t15.2023.12.10 val PER: 1.3206
2025-11-06 01:31:17,430: t15.2023.12.17 val PER: 1.0686
2025-11-06 01:31:17,430: t15.2023.12.29 val PER: 1.1627
2025-11-06 01:31:17,431: t15.2024.02.25 val PER: 1.1320
2025-11-06 01:31:17,431: t15.2024.03.08 val PER: 1.1550
2025-11-06 01:31:17,431: t15.2024.03.15 val PER: 1.1138
2025-11-06 01:31:17,431: t15.2024.03.17 val PER: 1.1702
2025-11-06 01:31:17,431: t15.2024.05.10 val PER: 1.2080
2025-11-06 01:31:17,432: t15.2024.06.14 val PER: 1.4117
2025-11-06 01:31:17,432: t15.2024.07.19 val PER: 0.9855
2025-11-06 01:31:17,433: t15.2024.07.21 val PER: 1.4207
2025-11-06 01:31:17,433: t15.2024.07.28 val PER: 1.4684
2025-11-06 01:31:17,433: t15.2025.01.10 val PER: 0.9518
2025-11-06 01:31:17,433: t15.2025.01.12 val PER: 1.4165
2025-11-06 01:31:17,433: t15.2025.03.14 val PER: 1.0192
2025-11-06 01:31:17,433: t15.2025.03.16 val PER: 1.4188
2025-11-06 01:31:17,434: t15.2025.03.30 val PER: 1.0805
2025-11-06 01:31:17,434: t15.2025.04.13 val PER: 1.3039
2025-11-06 01:31:17,435: New best test PER inf --> 1.2153
2025-11-06 01:31:17,435: Checkpointing model
2025-11-06 01:31:25,114: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-11-06 01:34:20,425: Train batch 200: loss: 94.31 grad norm: 48.85 time: 0.555
2025-11-06 01:37:16,121: Train batch 400: loss: 74.43 grad norm: 51.56 time: 0.748
2025-11-06 01:40:07,050: Train batch 600: loss: 59.02 grad norm: 59.80 time: 0.542
2025-11-06 01:42:55,463: Train batch 800: loss: 51.41 grad norm: 48.43 time: 0.731
2025-11-06 01:45:39,964: Train batch 1000: loss: 44.69 grad norm: 44.89 time: 0.696
2025-11-06 01:48:19,243: Train batch 1200: loss: 27.98 grad norm: 36.67 time: 0.734
2025-11-06 01:51:14,321: Train batch 1400: loss: 17.07 grad norm: 27.18 time: 0.678
2025-11-06 01:54:13,208: Train batch 1600: loss: 29.28 grad norm: 42.53 time: 0.709
2025-11-06 01:57:09,754: Train batch 1800: loss: 25.32 grad norm: 42.65 time: 0.541
2025-11-06 02:00:15,113: Train batch 2000: loss: 11.38 grad norm: 26.34 time: 0.673
2025-11-06 02:00:15,116: Running test after training batch: 2000
2025-11-06 02:00:41,197: Val batch 2000: PER (avg): 0.2245 CTC Loss (avg): 22.6278 time: 26.081
2025-11-06 02:00:41,198: t15.2023.08.13 val PER: 0.1861
2025-11-06 02:00:41,198: t15.2023.08.18 val PER: 0.1777
2025-11-06 02:00:41,198: t15.2023.08.20 val PER: 0.1724
2025-11-06 02:00:41,198: t15.2023.08.25 val PER: 0.1521
2025-11-06 02:00:41,198: t15.2023.08.27 val PER: 0.2621
2025-11-06 02:00:41,198: t15.2023.09.01 val PER: 0.1429
2025-11-06 02:00:41,198: t15.2023.09.03 val PER: 0.2280
2025-11-06 02:00:41,198: t15.2023.09.24 val PER: 0.1796
2025-11-06 02:00:41,198: t15.2023.09.29 val PER: 0.1908
2025-11-06 02:00:41,199: t15.2023.10.01 val PER: 0.2345
2025-11-06 02:00:41,199: t15.2023.10.06 val PER: 0.1636
2025-11-06 02:00:41,199: t15.2023.10.08 val PER: 0.3139
2025-11-06 02:00:41,199: t15.2023.10.13 val PER: 0.2964
2025-11-06 02:00:41,199: t15.2023.10.15 val PER: 0.2274
2025-11-06 02:00:41,199: t15.2023.10.20 val PER: 0.2282
2025-11-06 02:00:41,199: t15.2023.10.22 val PER: 0.1904
2025-11-06 02:00:41,199: t15.2023.11.03 val PER: 0.2266
2025-11-06 02:00:41,199: t15.2023.11.04 val PER: 0.0580
2025-11-06 02:00:41,200: t15.2023.11.17 val PER: 0.0918
2025-11-06 02:00:41,200: t15.2023.11.19 val PER: 0.1038
2025-11-06 02:00:41,200: t15.2023.11.26 val PER: 0.2464
2025-11-06 02:00:41,200: t15.2023.12.03 val PER: 0.1922
2025-11-06 02:00:41,200: t15.2023.12.08 val PER: 0.2011
2025-11-06 02:00:41,200: t15.2023.12.10 val PER: 0.1656
2025-11-06 02:00:41,200: t15.2023.12.17 val PER: 0.2110
2025-11-06 02:00:41,200: t15.2023.12.29 val PER: 0.2107
2025-11-06 02:00:41,200: t15.2024.02.25 val PER: 0.1840
2025-11-06 02:00:41,201: t15.2024.03.08 val PER: 0.3030
2025-11-06 02:00:41,201: t15.2024.03.15 val PER: 0.2883
2025-11-06 02:00:41,201: t15.2024.03.17 val PER: 0.2329
2025-11-06 02:00:41,201: t15.2024.05.10 val PER: 0.2259
2025-11-06 02:00:41,201: t15.2024.06.14 val PER: 0.2382
2025-11-06 02:00:41,201: t15.2024.07.19 val PER: 0.3098
2025-11-06 02:00:41,201: t15.2024.07.21 val PER: 0.1510
2025-11-06 02:00:41,201: t15.2024.07.28 val PER: 0.2081
2025-11-06 02:00:41,201: t15.2025.01.10 val PER: 0.3953
2025-11-06 02:00:41,201: t15.2025.01.12 val PER: 0.2309
2025-11-06 02:00:41,202: t15.2025.03.14 val PER: 0.3891
2025-11-06 02:00:41,202: t15.2025.03.16 val PER: 0.2670
2025-11-06 02:00:41,202: t15.2025.03.30 val PER: 0.3678
2025-11-06 02:00:41,202: t15.2025.04.13 val PER: 0.2725
2025-11-06 02:00:41,202: New best test PER 1.2153 --> 0.2245
2025-11-06 02:00:41,202: Checkpointing model
2025-11-06 02:00:47,851: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-11-06 02:03:34,936: Train batch 2200: loss: 17.52 grad norm: 39.04 time: 0.640
2025-11-06 02:06:17,999: Train batch 2400: loss: 16.03 grad norm: 34.45 time: 0.655
2025-11-06 02:09:06,649: Train batch 2600: loss: 9.96 grad norm: 24.48 time: 0.584
2025-11-06 02:11:53,602: Train batch 2800: loss: 10.07 grad norm: 26.51 time: 0.924
2025-11-06 02:14:39,085: Train batch 3000: loss: 13.36 grad norm: 32.69 time: 0.609
2025-11-06 02:17:24,780: Train batch 3200: loss: 10.44 grad norm: 28.63 time: 0.499
2025-11-06 02:20:10,886: Train batch 3400: loss: 7.12 grad norm: 31.68 time: 0.765
2025-11-06 02:22:57,839: Train batch 3600: loss: 7.13 grad norm: 25.57 time: 0.679
2025-11-06 02:25:44,149: Train batch 3800: loss: 7.45 grad norm: 31.57 time: 0.701
2025-11-06 02:28:32,254: Train batch 4000: loss: 4.62 grad norm: 21.20 time: 0.651
2025-11-06 02:28:32,262: Running test after training batch: 4000
2025-11-06 02:29:00,986: Val batch 4000: PER (avg): 0.1603 CTC Loss (avg): 17.7552 time: 28.724
2025-11-06 02:29:00,986: t15.2023.08.13 val PER: 0.1247
2025-11-06 02:29:00,987: t15.2023.08.18 val PER: 0.1266
2025-11-06 02:29:00,987: t15.2023.08.20 val PER: 0.0961
2025-11-06 02:29:00,987: t15.2023.08.25 val PER: 0.1024
2025-11-06 02:29:00,987: t15.2023.08.27 val PER: 0.1961
2025-11-06 02:29:00,987: t15.2023.09.01 val PER: 0.0787
2025-11-06 02:29:00,987: t15.2023.09.03 val PER: 0.1544
2025-11-06 02:29:00,987: t15.2023.09.24 val PER: 0.1250
2025-11-06 02:29:00,987: t15.2023.09.29 val PER: 0.1423
2025-11-06 02:29:00,987: t15.2023.10.01 val PER: 0.1737
2025-11-06 02:29:00,988: t15.2023.10.06 val PER: 0.1270
2025-11-06 02:29:00,988: t15.2023.10.08 val PER: 0.2679
2025-11-06 02:29:00,988: t15.2023.10.13 val PER: 0.2374
2025-11-06 02:29:00,988: t15.2023.10.15 val PER: 0.1694
2025-11-06 02:29:00,988: t15.2023.10.20 val PER: 0.2114
2025-11-06 02:29:00,988: t15.2023.10.22 val PER: 0.1303
2025-11-06 02:29:00,988: t15.2023.11.03 val PER: 0.1845
2025-11-06 02:29:00,988: t15.2023.11.04 val PER: 0.0239
2025-11-06 02:29:00,988: t15.2023.11.17 val PER: 0.0389
2025-11-06 02:29:00,988: t15.2023.11.19 val PER: 0.0579
2025-11-06 02:29:00,989: t15.2023.11.26 val PER: 0.1268
2025-11-06 02:29:00,989: t15.2023.12.03 val PER: 0.1145
2025-11-06 02:29:00,989: t15.2023.12.08 val PER: 0.1072
2025-11-06 02:29:00,989: t15.2023.12.10 val PER: 0.0999
2025-11-06 02:29:00,989: t15.2023.12.17 val PER: 0.1518
2025-11-06 02:29:00,989: t15.2023.12.29 val PER: 0.1338
2025-11-06 02:29:00,989: t15.2024.02.25 val PER: 0.1180
2025-11-06 02:29:00,989: t15.2024.03.08 val PER: 0.2461
2025-11-06 02:29:00,989: t15.2024.03.15 val PER: 0.2114
2025-11-06 02:29:00,989: t15.2024.03.17 val PER: 0.1444
2025-11-06 02:29:00,990: t15.2024.05.10 val PER: 0.1620
2025-11-06 02:29:00,990: t15.2024.06.14 val PER: 0.1625
2025-11-06 02:29:00,990: t15.2024.07.19 val PER: 0.2413
2025-11-06 02:29:00,990: t15.2024.07.21 val PER: 0.0890
2025-11-06 02:29:00,990: t15.2024.07.28 val PER: 0.1581
2025-11-06 02:29:00,990: t15.2025.01.10 val PER: 0.3099
2025-11-06 02:29:00,990: t15.2025.01.12 val PER: 0.1655
2025-11-06 02:29:00,991: t15.2025.03.14 val PER: 0.3595
2025-11-06 02:29:00,991: t15.2025.03.16 val PER: 0.2029
2025-11-06 02:29:00,991: t15.2025.03.30 val PER: 0.2954
2025-11-06 02:29:00,991: t15.2025.04.13 val PER: 0.2325
2025-11-06 02:29:00,991: New best test PER 0.2245 --> 0.1603
2025-11-06 02:29:00,991: Checkpointing model
2025-11-06 02:29:02,861: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-11-06 02:31:49,446: Train batch 4200: loss: 4.89 grad norm: 22.97 time: 0.714
2025-11-06 02:34:37,615: Train batch 4400: loss: 8.12 grad norm: 30.75 time: 0.821
2025-11-06 02:37:24,937: Train batch 4600: loss: 4.16 grad norm: 23.44 time: 0.604
2025-11-06 02:40:12,871: Train batch 4800: loss: 7.91 grad norm: 31.35 time: 0.662
2025-11-06 02:43:01,363: Train batch 5000: loss: 4.94 grad norm: 25.72 time: 0.808
2025-11-06 02:45:47,482: Train batch 5200: loss: 6.68 grad norm: 28.85 time: 0.651
2025-11-06 02:48:35,749: Train batch 5400: loss: 2.00 grad norm: 17.02 time: 0.744
2025-11-06 02:51:19,919: Train batch 5600: loss: 5.33 grad norm: 27.69 time: 0.654
2025-11-06 02:54:04,394: Train batch 5800: loss: 3.40 grad norm: 21.54 time: 0.626
2025-11-06 02:56:49,964: Train batch 6000: loss: 4.05 grad norm: 23.31 time: 0.774
2025-11-06 02:56:49,967: Running test after training batch: 6000
2025-11-06 02:57:18,697: Val batch 6000: PER (avg): 0.1391 CTC Loss (avg): 17.3393 time: 28.729
2025-11-06 02:57:18,698: t15.2023.08.13 val PER: 0.1091
2025-11-06 02:57:18,698: t15.2023.08.18 val PER: 0.0997
2025-11-06 02:57:18,699: t15.2023.08.20 val PER: 0.0945
2025-11-06 02:57:18,699: t15.2023.08.25 val PER: 0.0934
2025-11-06 02:57:18,699: t15.2023.08.27 val PER: 0.1736
2025-11-06 02:57:18,699: t15.2023.09.01 val PER: 0.0722
2025-11-06 02:57:18,699: t15.2023.09.03 val PER: 0.1354
2025-11-06 02:57:18,699: t15.2023.09.24 val PER: 0.1068
2025-11-06 02:57:18,699: t15.2023.09.29 val PER: 0.1244
2025-11-06 02:57:18,699: t15.2023.10.01 val PER: 0.1605
2025-11-06 02:57:18,699: t15.2023.10.06 val PER: 0.1087
2025-11-06 02:57:18,700: t15.2023.10.08 val PER: 0.2192
2025-11-06 02:57:18,700: t15.2023.10.13 val PER: 0.2048
2025-11-06 02:57:18,700: t15.2023.10.15 val PER: 0.1365
2025-11-06 02:57:18,700: t15.2023.10.20 val PER: 0.1812
2025-11-06 02:57:18,700: t15.2023.10.22 val PER: 0.1147
2025-11-06 02:57:18,700: t15.2023.11.03 val PER: 0.1723
2025-11-06 02:57:18,700: t15.2023.11.04 val PER: 0.0137
2025-11-06 02:57:18,700: t15.2023.11.17 val PER: 0.0295
2025-11-06 02:57:18,700: t15.2023.11.19 val PER: 0.0319
2025-11-06 02:57:18,701: t15.2023.11.26 val PER: 0.1080
2025-11-06 02:57:18,701: t15.2023.12.03 val PER: 0.0966
2025-11-06 02:57:18,701: t15.2023.12.08 val PER: 0.0826
2025-11-06 02:57:18,701: t15.2023.12.10 val PER: 0.0841
2025-11-06 02:57:18,701: t15.2023.12.17 val PER: 0.1279
2025-11-06 02:57:18,701: t15.2023.12.29 val PER: 0.1153
2025-11-06 02:57:18,701: t15.2024.02.25 val PER: 0.1152
2025-11-06 02:57:18,701: t15.2024.03.08 val PER: 0.2333
2025-11-06 02:57:18,701: t15.2024.03.15 val PER: 0.2045
2025-11-06 02:57:18,702: t15.2024.03.17 val PER: 0.1248
2025-11-06 02:57:18,702: t15.2024.05.10 val PER: 0.1501
2025-11-06 02:57:18,702: t15.2024.06.14 val PER: 0.1325
2025-11-06 02:57:18,702: t15.2024.07.19 val PER: 0.1978
2025-11-06 02:57:18,702: t15.2024.07.21 val PER: 0.0821
2025-11-06 02:57:18,702: t15.2024.07.28 val PER: 0.1279
2025-11-06 02:57:18,702: t15.2025.01.10 val PER: 0.2521
2025-11-06 02:57:18,702: t15.2025.01.12 val PER: 0.1393
2025-11-06 02:57:18,702: t15.2025.03.14 val PER: 0.3225
2025-11-06 02:57:18,703: t15.2025.03.16 val PER: 0.1780
2025-11-06 02:57:18,703: t15.2025.03.30 val PER: 0.2460
2025-11-06 02:57:18,703: t15.2025.04.13 val PER: 0.2140
2025-11-06 02:57:18,703: New best test PER 0.1603 --> 0.1391
2025-11-06 02:57:18,703: Checkpointing model
2025-11-06 02:57:23,940: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-11-06 03:00:10,050: Train batch 6200: loss: 4.62 grad norm: 28.69 time: 0.796
2025-11-06 03:03:00,747: Train batch 6400: loss: 6.16 grad norm: 27.78 time: 0.790
2025-11-06 03:05:50,756: Train batch 6600: loss: 6.08 grad norm: 35.39 time: 0.738
2025-11-06 03:08:38,915: Train batch 6800: loss: 3.45 grad norm: 25.24 time: 0.461
2025-11-06 03:11:26,987: Train batch 7000: loss: 4.45 grad norm: 20.00 time: 0.773
2025-11-06 03:14:12,008: Train batch 7200: loss: 2.19 grad norm: 19.41 time: 0.808
2025-11-06 03:17:04,209: Train batch 7400: loss: 4.45 grad norm: 28.03 time: 0.868
2025-11-06 03:19:51,873: Train batch 7600: loss: 2.45 grad norm: 23.39 time: 0.756
2025-11-06 03:22:39,236: Train batch 7800: loss: 3.23 grad norm: 22.75 time: 1.046
2025-11-06 03:25:26,052: Train batch 8000: loss: 2.46 grad norm: 18.75 time: 0.546
2025-11-06 03:25:26,056: Running test after training batch: 8000
2025-11-06 03:25:54,571: Val batch 8000: PER (avg): 0.1324 CTC Loss (avg): 17.9034 time: 28.515
2025-11-06 03:25:54,572: t15.2023.08.13 val PER: 0.1060
2025-11-06 03:25:54,572: t15.2023.08.18 val PER: 0.0956
2025-11-06 03:25:54,572: t15.2023.08.20 val PER: 0.0770
2025-11-06 03:25:54,572: t15.2023.08.25 val PER: 0.0949
2025-11-06 03:25:54,572: t15.2023.08.27 val PER: 0.1897
2025-11-06 03:25:54,572: t15.2023.09.01 val PER: 0.0568
2025-11-06 03:25:54,573: t15.2023.09.03 val PER: 0.1259
2025-11-06 03:25:54,573: t15.2023.09.24 val PER: 0.1117
2025-11-06 03:25:54,573: t15.2023.09.29 val PER: 0.1168
2025-11-06 03:25:54,573: t15.2023.10.01 val PER: 0.1572
2025-11-06 03:25:54,573: t15.2023.10.06 val PER: 0.1012
2025-11-06 03:25:54,573: t15.2023.10.08 val PER: 0.2124
2025-11-06 03:25:54,573: t15.2023.10.13 val PER: 0.1808
2025-11-06 03:25:54,573: t15.2023.10.15 val PER: 0.1299
2025-11-06 03:25:54,574: t15.2023.10.20 val PER: 0.2013
2025-11-06 03:25:54,574: t15.2023.10.22 val PER: 0.1147
2025-11-06 03:25:54,574: t15.2023.11.03 val PER: 0.1560
2025-11-06 03:25:54,574: t15.2023.11.04 val PER: 0.0205
2025-11-06 03:25:54,574: t15.2023.11.17 val PER: 0.0264
2025-11-06 03:25:54,574: t15.2023.11.19 val PER: 0.0379
2025-11-06 03:25:54,575: t15.2023.11.26 val PER: 0.0920
2025-11-06 03:25:54,575: t15.2023.12.03 val PER: 0.0966
2025-11-06 03:25:54,575: t15.2023.12.08 val PER: 0.0686
2025-11-06 03:25:54,575: t15.2023.12.10 val PER: 0.0604
2025-11-06 03:25:54,575: t15.2023.12.17 val PER: 0.1133
2025-11-06 03:25:54,575: t15.2023.12.29 val PER: 0.0981
2025-11-06 03:25:54,575: t15.2024.02.25 val PER: 0.0955
2025-11-06 03:25:54,575: t15.2024.03.08 val PER: 0.1863
2025-11-06 03:25:54,575: t15.2024.03.15 val PER: 0.1901
2025-11-06 03:25:54,576: t15.2024.03.17 val PER: 0.1199
2025-11-06 03:25:54,576: t15.2024.05.10 val PER: 0.1367
2025-11-06 03:25:54,576: t15.2024.06.14 val PER: 0.1404
2025-11-06 03:25:54,576: t15.2024.07.19 val PER: 0.2017
2025-11-06 03:25:54,576: t15.2024.07.21 val PER: 0.0779
2025-11-06 03:25:54,576: t15.2024.07.28 val PER: 0.1140
2025-11-06 03:25:54,576: t15.2025.01.10 val PER: 0.2893
2025-11-06 03:25:54,576: t15.2025.01.12 val PER: 0.1324
2025-11-06 03:25:54,576: t15.2025.03.14 val PER: 0.3225
2025-11-06 03:25:54,576: t15.2025.03.16 val PER: 0.2016
2025-11-06 03:25:54,577: t15.2025.03.30 val PER: 0.2632
2025-11-06 03:25:54,577: t15.2025.04.13 val PER: 0.2126
2025-11-06 03:25:54,577: New best test PER 0.1391 --> 0.1324
2025-11-06 03:25:54,577: Checkpointing model
2025-11-06 03:26:25,760: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-11-06 03:29:10,939: Train batch 8200: loss: 3.18 grad norm: 24.49 time: 0.723
2025-11-06 03:31:57,256: Train batch 8400: loss: 4.45 grad norm: 49.57 time: 0.735
2025-11-06 03:34:47,032: Train batch 8600: loss: 2.43 grad norm: 23.98 time: 0.567
2025-11-06 03:37:34,566: Train batch 8800: loss: 3.14 grad norm: 31.88 time: 0.516
2025-11-06 03:40:23,185: Train batch 9000: loss: 1.99 grad norm: 17.04 time: 0.461
2025-11-06 03:43:14,830: Train batch 9200: loss: 2.08 grad norm: 22.94 time: 0.633
2025-11-06 03:46:05,903: Train batch 9400: loss: 1.23 grad norm: 14.13 time: 1.077
2025-11-06 03:48:57,436: Train batch 9600: loss: 2.43 grad norm: 25.04 time: 0.633
2025-11-06 03:51:46,312: Train batch 9800: loss: 2.39 grad norm: 50.95 time: 0.572
2025-11-06 03:54:33,075: Train batch 10000: loss: 3.30 grad norm: 24.25 time: 0.721
2025-11-06 03:54:33,080: Running test after training batch: 10000
2025-11-06 03:55:01,205: Val batch 10000: PER (avg): 0.1286 CTC Loss (avg): 19.6356 time: 28.123
2025-11-06 03:55:01,206: t15.2023.08.13 val PER: 0.1040
2025-11-06 03:55:01,206: t15.2023.08.18 val PER: 0.0863
2025-11-06 03:55:01,206: t15.2023.08.20 val PER: 0.0770
2025-11-06 03:55:01,206: t15.2023.08.25 val PER: 0.0873
2025-11-06 03:55:01,206: t15.2023.08.27 val PER: 0.1752
2025-11-06 03:55:01,206: t15.2023.09.01 val PER: 0.0609
2025-11-06 03:55:01,206: t15.2023.09.03 val PER: 0.1211
2025-11-06 03:55:01,207: t15.2023.09.24 val PER: 0.0862
2025-11-06 03:55:01,207: t15.2023.09.29 val PER: 0.1142
2025-11-06 03:55:01,207: t15.2023.10.01 val PER: 0.1539
2025-11-06 03:55:01,207: t15.2023.10.06 val PER: 0.0883
2025-11-06 03:55:01,207: t15.2023.10.08 val PER: 0.2070
2025-11-06 03:55:01,207: t15.2023.10.13 val PER: 0.1854
2025-11-06 03:55:01,207: t15.2023.10.15 val PER: 0.1318
2025-11-06 03:55:01,207: t15.2023.10.20 val PER: 0.2114
2025-11-06 03:55:01,207: t15.2023.10.22 val PER: 0.1136
2025-11-06 03:55:01,207: t15.2023.11.03 val PER: 0.1486
2025-11-06 03:55:01,208: t15.2023.11.04 val PER: 0.0137
2025-11-06 03:55:01,208: t15.2023.11.17 val PER: 0.0264
2025-11-06 03:55:01,208: t15.2023.11.19 val PER: 0.0319
2025-11-06 03:55:01,208: t15.2023.11.26 val PER: 0.0768
2025-11-06 03:55:01,208: t15.2023.12.03 val PER: 0.1050
2025-11-06 03:55:01,208: t15.2023.12.08 val PER: 0.0686
2025-11-06 03:55:01,208: t15.2023.12.10 val PER: 0.0486
2025-11-06 03:55:01,208: t15.2023.12.17 val PER: 0.1154
2025-11-06 03:55:01,208: t15.2023.12.29 val PER: 0.0968
2025-11-06 03:55:01,209: t15.2024.02.25 val PER: 0.0801
2025-11-06 03:55:01,209: t15.2024.03.08 val PER: 0.2063
2025-11-06 03:55:01,209: t15.2024.03.15 val PER: 0.2026
2025-11-06 03:55:01,209: t15.2024.03.17 val PER: 0.1116
2025-11-06 03:55:01,209: t15.2024.05.10 val PER: 0.1426
2025-11-06 03:55:01,209: t15.2024.06.14 val PER: 0.1120
2025-11-06 03:55:01,209: t15.2024.07.19 val PER: 0.1839
2025-11-06 03:55:01,209: t15.2024.07.21 val PER: 0.0703
2025-11-06 03:55:01,209: t15.2024.07.28 val PER: 0.1088
2025-11-06 03:55:01,209: t15.2025.01.10 val PER: 0.2727
2025-11-06 03:55:01,210: t15.2025.01.12 val PER: 0.1309
2025-11-06 03:55:01,210: t15.2025.03.14 val PER: 0.3432
2025-11-06 03:55:01,210: t15.2025.03.16 val PER: 0.2016
2025-11-06 03:55:01,210: t15.2025.03.30 val PER: 0.2575
2025-11-06 03:55:01,210: t15.2025.04.13 val PER: 0.2040
2025-11-06 03:55:01,210: New best test PER 0.1324 --> 0.1286
2025-11-06 03:55:01,210: Checkpointing model
2025-11-06 03:55:22,414: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
